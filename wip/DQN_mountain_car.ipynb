{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaeloneill/Documents/RL/venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib.tile_coding import IHT, tiles\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None, num_tilings=8, max_size=4096):\n",
    "        self.scope = scope\n",
    "        self.max_size = max_size\n",
    "        self.num_tilings = num_tilings\n",
    "\n",
    "        self.iht = IHT(max_size)\n",
    "        \n",
    "         # position and velocity needs scaling to satisfy the tile software\n",
    "        self.position_scale = self.num_tilings / (env.observation_space.high[0] - env.observation_space.low[0])\n",
    "        self.velocity_scale = self.num_tilings / (env.observation_space.high[1] - env.observation_space.low[1])\n",
    "        \n",
    "        \n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def featurize_state_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a state.\n",
    "        \"\"\"\n",
    "        featurized = tiles(self.iht, self.num_tilings, [self.position_scale * state[0], self.velocity_scale * state[1]], [action])\n",
    "       \n",
    "        return featurized\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, self.num_tilings], dtype=tf.float32, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        #X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.floordiv(tf.shape(self.X_pl)[0], 3)\n",
    "        \n",
    "        fc1 = tf.contrib.layers.fully_connected(self.X_pl, 100)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(\n",
    "            fc1, 1, activation_fn=None)\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        \n",
    "        gather_indices = tf.range(batch_size) * 3 + self.actions_pl\n",
    "        self.action_predictions = tf.squeeze(tf.gather(self.predictions, gather_indices), axis=1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "        #self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s, a=None):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        if a is None:\n",
    "            for s_ in s:\n",
    "                features.extend([self.featurize_state_action(s_, i) for i in range(env.action_space.n)])\n",
    "        else:\n",
    "            raise ValueError\n",
    "            for s_ in s:\n",
    "                features.extend([self.featurize_state_action(s, a)])\n",
    "        return sess.run(self.predictions, {self.X_pl: np.array(features)})\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "     \n",
    "        features = []\n",
    "        for s_ in s:\n",
    "            features.extend([self.featurize_state_action(s_, i) for i in range(env.action_space.n)])\n",
    "        \n",
    "        feed_dict = { self.X_pl: features, self.y_pl: y, self.actions_pl: a}\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-936f3ac3ee9b>:69: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "[[-0.9678454]\n",
      " [-4.6341457]\n",
      " [-8.2629175]\n",
      " [-0.9678454]\n",
      " [-4.6341457]\n",
      " [-8.2629175]]\n",
      "167.22592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaeloneill/Documents/RL/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    #observation_p = sp.process(sess, observation)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10, 10])\n",
    "    a = np.array([0, 1])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.9,\n",
    "                    epsilon_start=1,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=50,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    #latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    #if latest_checkpoint:\n",
    "    #    print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "    #    saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator, 3)\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    #state = state_processor.process(sess, state)\n",
    "    #state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "       \n",
    "        #next_state = state_processor.process(sess, next_state)\n",
    "        #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            #state = state_processor.process(sess, state)\n",
    "            #state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    #env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        #state = state_processor.process(sess, state)\n",
    "        #state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            #next_state = state_processor.process(sess, next_state)\n",
    "            #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "         \n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)  \n",
    "            q_values_next = q_values_next.reshape((-1, 3))\n",
    "            \n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            #states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "        \n",
    "        # Print out which step we're on, useful for debugging.\n",
    "        sys.stdout.write(\"\\rEpisode {}/{}, loss: {}, reward: {}, epsilon: {}\".format(\n",
    "                i_episode + 1, num_episodes, loss, stats.episode_rewards[i_episode], epsilon))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaeloneill/Documents/RL/venv/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Episode 50/10000, loss: 122.32513427734375, reward: -200.0, epsilon: 0.9820899641799283\n",
      "Copied model parameters to target network.\n",
      "Episode 100/10000, loss: 5.799839496612549, reward: -200.0, epsilon: 0.9641799283598567\n",
      "Copied model parameters to target network.\n",
      "Episode 150/10000, loss: 2.7823326587677, reward: -200.0, epsilon: 0.9462698925397851\n",
      "Copied model parameters to target network.\n",
      "Episode 201/10000, loss: 0.13712400197982788, reward: -200.0, epsilon: 0.928001656003312\n",
      "Copied model parameters to target network.\n",
      "Episode 251/10000, loss: 0.11807287484407425, reward: -200.0, epsilon: 0.9100916201832404\n",
      "Copied model parameters to target network.\n",
      "Episode 301/10000, loss: 3.968113660812378, reward: -200.0, epsilon: 0.8921815843631687\n",
      "Copied model parameters to target network.\n",
      "Episode 351/10000, loss: 3.379532814025879, reward: -200.0, epsilon: 0.8742715485430971\n",
      "Copied model parameters to target network.\n",
      "Episode 402/10000, loss: 11.019857406616211, reward: -200.0, epsilon: 0.856003312006624\n",
      "Copied model parameters to target network.\n",
      "Episode 452/10000, loss: 14.060222625732422, reward: -200.0, epsilon: 0.8380932761865524\n",
      "Copied model parameters to target network.\n",
      "Episode 502/10000, loss: 1.1235146522521973, reward: -200.0, epsilon: 0.8201832403664807\n",
      "Copied model parameters to target network.\n",
      "Episode 552/10000, loss: 2.65614652633667, reward: -200.0, epsilon: 0.8022732045464092\n",
      "Copied model parameters to target network.\n",
      "Episode 603/10000, loss: 0.14666487276554108, reward: -200.0, epsilon: 0.784004968009936\n",
      "Copied model parameters to target network.\n",
      "Episode 653/10000, loss: 21.904502868652344, reward: -200.0, epsilon: 0.7660949321898644\n",
      "Copied model parameters to target network.\n",
      "Episode 703/10000, loss: 0.1462402194738388, reward: -200.0, epsilon: 0.7481848963697928\n",
      "Copied model parameters to target network.\n",
      "Episode 753/10000, loss: 3.3814077377319336, reward: -200.0, epsilon: 0.7302748605497211\n",
      "Copied model parameters to target network.\n",
      "Episode 804/10000, loss: 10.465616226196289, reward: -200.0, epsilon: 0.7120066240132481\n",
      "Copied model parameters to target network.\n",
      "Episode 854/10000, loss: 12.000768661499023, reward: -200.0, epsilon: 0.6940965881931764\n",
      "Copied model parameters to target network.\n",
      "Episode 904/10000, loss: 0.13415083289146423, reward: -200.0, epsilon: 0.6761865523731048\n",
      "Copied model parameters to target network.\n",
      "Episode 954/10000, loss: 0.13721249997615814, reward: -200.0, epsilon: 0.6582765165530331\n",
      "Copied model parameters to target network.\n",
      "Episode 1005/10000, loss: 16.153501510620117, reward: -200.0, epsilon: 0.6400082800165601\n",
      "Copied model parameters to target network.\n",
      "Episode 1055/10000, loss: 0.2357204407453537, reward: -200.0, epsilon: 0.6220982441964884\n",
      "Copied model parameters to target network.\n",
      "Episode 1105/10000, loss: 9.151671409606934, reward: -200.0, epsilon: 0.6041882083764167\n",
      "Copied model parameters to target network.\n",
      "Episode 1155/10000, loss: 0.13482873141765594, reward: -200.0, epsilon: 0.5862781725563451\n",
      "Copied model parameters to target network.\n",
      "Episode 1206/10000, loss: 1.4606127738952637, reward: -200.0, epsilon: 0.568009936019872\n",
      "Copied model parameters to target network.\n",
      "Episode 1256/10000, loss: 0.3358481526374817, reward: -200.0, epsilon: 0.5500999001998004\n",
      "Copied model parameters to target network.\n",
      "Episode 1306/10000, loss: 12.255131721496582, reward: -200.0, epsilon: 0.5321898643797287\n",
      "Copied model parameters to target network.\n",
      "Episode 1356/10000, loss: 3.971586227416992, reward: -200.0, epsilon: 0.514279828559657\n",
      "Copied model parameters to target network.\n",
      "Episode 1407/10000, loss: 11.026715278625488, reward: -200.0, epsilon: 0.496011592023184\n",
      "Copied model parameters to target network.\n",
      "Episode 1457/10000, loss: 2.046464443206787, reward: -200.0, epsilon: 0.47810155620311234\n",
      "Copied model parameters to target network.\n",
      "Episode 1507/10000, loss: 0.1169360801577568, reward: -200.0, epsilon: 0.4601915203830408\n",
      "Copied model parameters to target network.\n",
      "Episode 1557/10000, loss: 12.623571395874023, reward: -200.0, epsilon: 0.4422814845629691\n",
      "Copied model parameters to target network.\n",
      "Episode 1608/10000, loss: 0.8974106311798096, reward: -200.0, epsilon: 0.4240132480264961\n",
      "Copied model parameters to target network.\n",
      "Episode 1658/10000, loss: 10.888771057128906, reward: -200.0, epsilon: 0.4061032122064244\n",
      "Copied model parameters to target network.\n",
      "Episode 1708/10000, loss: 0.10650908201932907, reward: -200.0, epsilon: 0.38819317638635276\n",
      "Copied model parameters to target network.\n",
      "Episode 1758/10000, loss: 14.525424003601074, reward: -200.0, epsilon: 0.3702831405662811\n",
      "Copied model parameters to target network.\n",
      "Episode 1809/10000, loss: 9.621417045593262, reward: -200.0, epsilon: 0.35201490402980806\n",
      "Copied model parameters to target network.\n",
      "Episode 1859/10000, loss: 0.11003232002258301, reward: -200.0, epsilon: 0.3341048682097364\n",
      "Copied model parameters to target network.\n",
      "Episode 1909/10000, loss: 2.0389554500579834, reward: -200.0, epsilon: 0.3161948323896647\n",
      "Copied model parameters to target network.\n",
      "Episode 1959/10000, loss: 15.433815002441406, reward: -200.0, epsilon: 0.29828479656959317\n",
      "Copied model parameters to target network.\n",
      "Episode 2010/10000, loss: 0.10922200977802277, reward: -200.0, epsilon: 0.28001656003312003\n",
      "Copied model parameters to target network.\n",
      "Episode 2060/10000, loss: 0.09369906783103943, reward: -200.0, epsilon: 0.26210652421304836\n",
      "Copied model parameters to target network.\n",
      "Episode 2110/10000, loss: 0.10992731153964996, reward: -200.0, epsilon: 0.2441964883929768\n",
      "Copied model parameters to target network.\n",
      "Episode 2160/10000, loss: 0.08552715927362442, reward: -200.0, epsilon: 0.22628645257290514\n",
      "Copied model parameters to target network.\n",
      "Episode 2211/10000, loss: 14.08978271484375, reward: -200.0, epsilon: 0.208018216036432\n",
      "Copied model parameters to target network.\n",
      "Episode 2261/10000, loss: 0.08748024702072144, reward: -200.0, epsilon: 0.19010818021636045\n",
      "Copied model parameters to target network.\n",
      "Episode 2311/10000, loss: 0.0948239117860794, reward: -200.0, epsilon: 0.17219814439628878\n",
      "Copied model parameters to target network.\n",
      "Episode 2361/10000, loss: 0.10652447491884232, reward: -200.0, epsilon: 0.1542881085762171\n",
      "Copied model parameters to target network.\n",
      "Episode 2412/10000, loss: 1.080206036567688, reward: -200.0, epsilon: 0.13601987203974408\n",
      "Copied model parameters to target network.\n",
      "Episode 2462/10000, loss: 0.09212562441825867, reward: -200.0, epsilon: 0.11810983621967241\n",
      "Copied model parameters to target network.\n",
      "Episode 2512/10000, loss: 9.168545722961426, reward: -200.0, epsilon: 0.10019980039960075\n",
      "Copied model parameters to target network.\n",
      "Episode 2562/10000, loss: 0.08724406361579895, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2613/10000, loss: 2.0350964069366455, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2663/10000, loss: 0.07219158113002777, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2713/10000, loss: 0.08128499239683151, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2763/10000, loss: 0.06850019097328186, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2814/10000, loss: 0.06906409561634064, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2864/10000, loss: 1.4275568723678589, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2914/10000, loss: 0.7893750071525574, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 2964/10000, loss: 12.524283409118652, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3015/10000, loss: 1.5467017889022827, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3065/10000, loss: 0.05475251376628876, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3115/10000, loss: 2.7939300537109375, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3165/10000, loss: 0.054707832634449005, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3216/10000, loss: 28.125097274780273, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3266/10000, loss: 0.06043879687786102, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3316/10000, loss: 11.075361251831055, reward: -200.0, epsilon: 0.1\n",
      "Copied model parameters to target network.\n",
      "Episode 3359/10000, loss: 0.048126913607120514, reward: -200.0, epsilon: 0.1"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a5f110a14b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=50):\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9958577c828e>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# Perform gradient descent update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m#states_batch = np.array(states_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-936f3ac3ee9b>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, s, a, y)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize_state_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-936f3ac3ee9b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize_state_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-936f3ac3ee9b>\u001b[0m in \u001b[0;36mfeaturize_state_action\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfeaturized\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mfeaturized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tilings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvelocity_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeaturized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RL/lib/tile_coding.py\u001b[0m in \u001b[0;36mtiles\u001b[0;34m(ihtORsize, numtilings, floats, ints, readonly)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mTiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtiling\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumtilings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtilingX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiling\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtiling\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "#state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=None,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.9,\n",
    "                                    batch_size=50):\n",
    "        pass\n",
    "\n",
    "            #print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Loading model checkpoint /Users/michaeloneill/Documents/RL/wip/experiments/MountainCar-v0/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /Users/michaeloneill/Documents/RL/wip/experiments/MountainCar-v0/checkpoints/model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "epsilon = 0\n",
    "#for i_episide in range(20):\n",
    "state = env.reset()\n",
    "with tf.Session() as sess:\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator, env.action_space.n)\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    for t in itertools.count():\n",
    "        #env.render()\n",
    "        time.sleep(0.01)\n",
    "        action_probs = policy(sess, state, epsilon)\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stats(episode_lengths=array([199., 199., 199., ..., 199., 199., 199.]), episode_rewards=array([-200., -200., -200., ..., -200., -200., -200.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3359"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0,    0, 3359,    0,    0,    0,    0]),\n",
       " array([-200.5, -200.4, -200.3, -200.2, -200.1, -200. , -199.9, -199.8,\n",
       "        -199.7, -199.6, -199.5]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(stats[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    0,    0,    0, 3359,    0,    0,    0,    0]),\n",
       " array([198.5, 198.6, 198.7, 198.8, 198.9, 199. , 199.1, 199.2, 199.3,\n",
       "        199.4, 199.5]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4532,   25,   19,   28,   23,   23,   14,    3,    4,    3]),\n",
       " array([-200. , -192.8, -185.6, -178.4, -171.2, -164. , -156.8, -149.6,\n",
       "        -142.4, -135.2, -128. ]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.histogram(stats.episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
