{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial: <br>Semi-gradient n-step Sarsa and Sarsa($\\lambda$)<br>Theory and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is an exciting area of A.I that offers something entirely different to supervised or unsupervised techniques. In RL, an 'agent' learns to interact with an environment in a way that maximises the reward it receives with respect to some task. RL is distinct from supervised learning, which specifies via labelled examples the correct way for a system to act in every situation (whereas in RL only the end goal is specified, but not how to achieve it). It is also distinct from unsupervised learning, which although not dependent on examples of correct behaviour either, is concerned with finding hidden structure in unlabelled data rather than maximising reward signals.\n",
    "\n",
    "This tutorial focuses on two important and widely used RL algorithms, semi-gradient n-step Sarsa and Sarsa($\\lambda$), as applied to the Mountain Car problem. These algorithms, aside from being useful, pull together a lot of the key concepts in RL and so provide a great way to learn about RL more generally. Value functions, policy iteration, on vs off-policy control, bootstrapping, tabular vs approximate solution methods, state featurization and eligibility traces can all be understood by studying these two algorithms.\n",
    "\n",
    "[Part 1](#theory) of the tutorial summarises the key theoretical concepts in RL that n-step Sarsa and Sarsa($\\lambda$) draw upon. [Part 2](#implementation) implements each algorithm and its associated dependencies. [Part 3](#experiments) compares the performance of each algorithm through a number of simulations. [Part 4](#conclusion) wraps up and provides direction for further study.\n",
    "\n",
    "The two main resources I drew upon to make this tutorial were [Sutton and Barto's Introduction to Reinforcement Learning](http://www.incompleteideas.net/book/bookdraft2017nov5.pdf) (imges were also taken from here) and [Denny Britz's excellent implementations](https://github.com/dennybritz/reinforcement-learning) of some of the algorithms therein. The implementation for the [Mountain Car environment](https://gym.openai.com/envs/MountainCarContinuous-v0/) was imported from the OpenAI Gym, and the tile coding software used for state featurization was also from Sutton and Barto, installed from [here](http://incompleteideas.net/tiles/tiles3.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 &nbsp; Theory <a id='theory'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first cover the key concepts in RL that n-step Sarsa and Sarsa($\\lambda$) draw upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 &nbsp; Value Functions <a id='value_functions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all reinforcement learning algorithms involve estimating value functions, functions of states (or state-actions) that quantify how good it is for an agent to be in a particular state (or state-action pair), where 'good' is defined in terms of the rewards expected to follow from that state (or state-action pair) in the future. The rewards that can be expected depend on which actions will be taken, and so value functions must be defined with respect to particular policies $\\pi(a | s)$, which map from states to actions.\n",
    "\n",
    "Formally, the state-value of $s$ under policy $\\pi$ is defined as the expected $\\textit{return}$ $G_t = \\sum_0^\\infty \\gamma^k R_{t+k+1}$ when starting from $S_t$ and following $\\pi$ thereafter: \n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$$ \n",
    "\n",
    "$R_t$ is the reward received at time step $t$ and $\\gamma$ is the discount rate, $ 0 \\leq \\gamma \\leq 1$, which controls the relative weighting between near and far-sighted rewards, and ensures that the sum is finite for continuing tasks (i.e. those that don't naturally break down into terminating episodes).\n",
    "\n",
    "Similarly, the action-value (a.k.a q-value) of (s, a) under policy $\\pi$ is defined as the expected return when taking action $a$ in state $s$ and following policy $\\pi$ thereafter: \n",
    "\n",
    "$$q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 &nbsp; Policy Evaluation and the Prediction Problem <a id='policy_evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a model of the environment's transition dynamics  $p(sâ€²,r|s,a)$ , i.e the probabilities of which states and actions lead to which next states and rewards, then the value functions can be computed analytically. However, most of the time the dynamics are not known and the value functions must instead be estimated from actual sample experience, giving rise to $V(S_t) \\approx v_\\pi(S_t)$ and $Q(S_t, A_t) \\approx q_\\pi(S_t, A_t)$. The general approach to computing these estimations is to have the agent follow $\\pi$ and maintain for each state (or state-action) a running average of the returns that follow. This boils down to iterating over an update rule with the following general form:\n",
    "\n",
    "$$NewEstimate \\gets OldEstimate + \\alpha\\ [Target - OldEstimate]$$\n",
    "\n",
    "$Target$ is just the return from the current state (or state-action). This could be the actual return $G_t$, or some estimation of it (as we shall see below). The name $Target$ is appropriate since it indicates the desirable direction in which to move. $\\alpha$ then indicates by how much to move in this direction. For an equally weighted average we would choose $\\alpha=1/n$, where $n$ is the number of times the given state (or state-action) has been visited so far. However, most reinforcement learning problems are non-stationary, meaning that the true state and state-action values change over time as the policy changes towards an optimal one (see below). In this case it makes more sense to give more weight to recent returns than long-past returns, which can be achieved by holding $\\alpha$ constant. The task of making the value estimates consistent with the current policy being followed is called the Prediction Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 &nbsp; Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy evaluation is concerned with making the state or action-values consistent with the current policy being followed. Policy improvement is concerned with adjusting the current policy to one that when followed leads to a higher return. \n",
    "\n",
    "Policy improvement requires an estimate of the action-value function $Q(S_t, A_t) \\approx q_\\pi(S_t, A_t)$ as opposed to an estimate of the state-value function $V(S_t) \\approx v_\\pi(S_t)$, assuming the environments transition dynamics are not known. This is because whilst $V(S_t)$ tells you which states have the highest values, it does not tell you which actions lead to which states (we can't look ahead). $Q(S_t, A_t)$, however, directly caches the value of taking a particular action in a particular state, and so it isn't necessary to look ahead to where that action leads in order to determine its value.\n",
    "\n",
    "Suppose then that we have determined a good approximation $Q$ to the action-value function $q_\\pi$ for an arbitrary policy $\\pi$ via policy evaluation. We can improve on this policy simply by selecting at each state $s$ the action that appears best according to $Q(s, a)$ i.e. by switching to a new policy $\\pi^\\prime$ that is 'greedy' with respect to $Q(s, a)$. The new greedy policy $\\pi^\\prime$ is guaranteed to be better than the original policy $\\pi$ for all $s$ and $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 &nbsp; Policy iteration and the Control Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a policy has been improved using $Q(S_t, A_t) \\approx  q_\\pi(S_t, A_t)$ to yield a better policy $\\pi^\\prime$, we can then evaluate this new policy to find $Q(S_t, A_t) \\approx  q_{\\pi^\\prime}(S_t, A_t)$, before improving it once again to yield an even better policy $\\pi^{\\prime \\prime}$. By alternating these two processes, policy improvement and policy evaluation, we achieve a sequence of monotonically improving policies and value functions that converge towards the optimal policy and value function, $\\pi_*$ and $q_*$. This is called policy iteration. \n",
    "\n",
    "In policy iteration, each process completes before the other begins. This is not necessary and can slow down learning. Generalized policy iteration (GPI) refers to the general idea of letting the policy improvement and evaluation processes interact at an arbitrary level of granularity. For example, policy evaluation could be truncated after a single sweep of the state set, or when the task is episodic, after just a single episode, before performing a step of policy improvement. The latter is the approach we take in the implementations of this tutorial. The task of learning optimal policies in general is called the Control Problem.\n",
    "\n",
    "<img src=\"images/policy_iteration.png\" width=\"180\" class=\"Center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 &nbsp; On Policy vs Off-Policy Control <a id='on_off_policy'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration is only guaranteed to converge if each action is selected infinitely often in the limit. This will not be the case in the approach described above, since by always making the policy greedy with the current action-value function, we prevent exploration of the non-greedy actions. This problem can be mitigated if exploring starts are enforced, where each state has a non-zero probability of being selected to start the episode. However, for most practical situations where episodes are simulated from real experience, this is an unlikely assumption to make.\n",
    "\n",
    "There are two other approaches to ensuring that all actions continue to be selected: on-policy and off-policy methods. In on-policy methods, \n",
    "we continually make the policy $\\epsilon$-greedy w.r.t the current value function rather than deterministically greedy. This means that most of the time the maximising action is selected, but with probability $\\epsilon$ it selects an action at random. Convergence is guaranteed, since all actions continue to be selected. However, convergence is not actually towards the optimal (greedy) policy, but rather to a nearby one that still explores. \n",
    "\n",
    "In off-policy methods we avoid the compromise of learning a policy that is only near-optimal by leveraging two separate policies, one that is continually made greedy w.r.t the current value function and so becomes the optimal policy (a.k.a the target policy), and another that is more exploratory and used to generate sample experience (a.k.a the behaviour policy). Importance sampling is the mechanism that allows us to learn about the target policy using data generated by the behaviour policy; we don't go into detail about this, since the examples in this tutorial use on-policy methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 &nbsp; Tabular Solution Methods <a id='tabular_methods'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we describe the core model-free methods for learning optimal policies and value functions when the state and action spaces are small enough for us to learn distinct values for each and every state (or state-action). In this setting the value functions can be represented as arrays, or tables, which is why these methods are called tabular solution methods. These methods are distinguished only by the target used in the [update rule](#policy_evaluation) above, and so this is what we focus the discussion on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1 &nbsp; Monte Carlo (MC) Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target formed in Monte Carlo methods is simply $G_t$, the complete sample return. The MC update for state-values is therefore:\n",
    "\n",
    "$$ V(S_t) \\gets V(S_t) + \\alpha[G_t - V(S_t)]$$\n",
    "\n",
    "Since each $G_t$ is an unbiased estimator of $v_\\pi(s)$, i.e. $\\mathbb{E}[G_t | S_t = s] = v_\\pi(S_t)$, the average of these returns will converge in the long run to $v_\\pi(s)$. However, since the selection of actions in an episode is stochastic, there will be a high variance amongst the returns. Also, because $G_t$ is only available at the end of an episode, we must wait until then before we can make an update (i.e. updates are 'off-line'). Both of these things can result in slow learning.\n",
    "\n",
    "For action-values we simply switch $V(S_t)$ for $Q(S_t, A_t)$ in the update rule:\n",
    "\n",
    "$$ Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha[G_t - Q(S_t, A_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 &nbsp; One-step Temporal Difference (TD) Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target formed in one-step TD methods is the truncated return $G_{t:t+1} = R_{t+1} + V(S_{t+1})$. In other words, $G_{t:t+1}$ is a re-estimation of the value of the current state made at the $\\textit{next}$ timestep $t+1$ (hence the name 'temporal difference'). The one-step TD update for state-values is thus:\n",
    "\n",
    "$$ V(S_t) \\gets V(S_t) + \\alpha[G_{t:t+1} - V(S_t)]$$\n",
    "\n",
    "Whilst the MC target was an unbiased estimate of $v_\\pi(S_t)$, this time it is biased, due to its dependence on the estimate $V(S_{t+1})$. Basing an estimate off of other estimates like this is called 'bootstrapping'. The payoff we get for introducing bias is lower variance, since the target only depends on one stochastic action selection. Also, whilst the MC target was only available at the end of the episode, this time it is available at $t+1$, since it only depends on the next state and reward. TD updates can therefore be performed 'on-line' (during an episode) which can result in faster learning (our value estimates are better sooner, enabling our agent to more immediately exploit anything that has changed).\n",
    "\n",
    "The fact that TD converges implies that the estimates of $v_\\pi(s)$ made at future time steps are better than those made at the current time step. This makes intuitive sense: if I asked you to estimate the length of your commute home as you got in the car, your re-estimate of that value once you've seen how much traffic there is on the motorway is likely to be more accurate.\n",
    "\n",
    "Once again for action-values we simply switch the $V$s for $Q$s in the update rule:\n",
    "\n",
    "$$ Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha[G_{t:t+1} - Q(S_t, A_t)] $$\n",
    "\n",
    "with $G_{t:t+1} = R_{t+1} + Q(S_{t+1}, A_{t+1})$. This action-value update depends on every element of the quintuple of events $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$, and so is referred to by the name one-step $\\textit{Sarsa}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.3 &nbsp; n-step TD Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-step TD methods generalize one-step TD presented above. Whilst one-step TD methods use the target $G_{t:t+1} = R_{t+1} + V(S_{t+1})$, n-step TD methods use the target $G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n})$. In other words, one-step TD updates bootstrap from $V(S_{t+1})$, whilst n-step updates bootstrap from $V(S_{t+n})$. The n-step TD update for state-values is therefore:\n",
    "\n",
    "$$ V(S_t) \\gets V(S_t) + \\alpha[G_{t:t+n} - V(S_t)]$$\n",
    "\n",
    "As usual we arrive at the corresponding update for action-values by swapping the $V$s for $Q$s:\n",
    "\n",
    "$$ Q(S_t, A_t) \\gets Q(S_t, A_t) + \\alpha[G_{t:t+n} - Q(S_t, A_t)],$$\n",
    "\n",
    "with $G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\ldots + \\gamma^{n-1} R_{t+n} + \\gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n})$. This action-value form is called n-step Sarsa, by analogy with one-step Sarsa above. \n",
    "\n",
    "n-step TD methods span a spectrum with one-step TD at one end (n=1) and MC at the other (n equal to the number of steps in the episode). Increasing n reduces the bias of the estimator (moving us closer towards the unbiased estimates of MC) but increases the variance, as well as the time we have to wait before we can make an update. Often intermediate values of n are are the most effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 &nbsp; Approximate Solution Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are very many states then it becomes impractical, both in terms of memory and time, to find accurate estimates of all states (or state-actions) like tabular methods. Instead we maintain our value function estimates as parameterized functions, $\\hat{v}(s, \\mathbf{w})$ and $\\hat{q}(s, a, \\mathbf{w})$, where $\\mathbf{w} \\in \\mathbb{R}^d$ is the weight vector, whose dimensionality $d$ is much less than the number of states. $\\mathbf{w}$ is learned via an update rule, in the same way that individual states and state-actions are in tabular methods. Because an update to the weight vector affects the values of many different states (and state-actions), approximate solution methods have the ability to generalise to previously unseen states (and state-actions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.1 &nbsp; Update Rule for Approximate Solution Methods <a id='approximate_update'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We borrow from supervised learning methodology in order to formulate the update rule for $\\mathbf{w}$, and thus learn the parameterised value functions $\\hat{v}(s, \\mathbf{w})$ and $\\hat{q}(s, a, \\mathbf{w})$. If we denote the update rule for state-values as $S_t \\to U_t$, where $S_t$ is the state whose value is to be updated and $U_t$ is a target that doesn't depend on $\\mathbf{w}$, then we can interpret $(S_t, U_t)$ as an input-output training example for $\\hat{v}$. If we use the mean squared error to represent prediction error for this example, $[U_t - \\hat{v}(S_t, \\mathbf{w})]^2$, then we can perform the weight vector update via stochastic gradient descent (SGD):\n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\mathbf{w}_{t+1} &= \\mathbf{w}_t - \\frac{1}{2} \\alpha \\nabla \\left[U_t - \\hat{v}(S_t, \\mathbf{w}_t) \\right]^2 \\\\\n",
    "&= \\mathbf{w}_t + \\alpha \\left[U_t - \\hat{v}(S_t, \\mathbf{w}_t) \\right] \\nabla \\hat{v}(S_t, \\mathbf{w}_t)\n",
    "\\end{aligned}$$\n",
    "\n",
    "We arrive at the corresponding update step for action-values by substituting the $\\hat{v}$s for $\\hat{q}$s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.2 &nbsp; Gradient and Semi-gradient methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the [tabular solution methods](#tabular_methods) has an approximate solution version that use the [gradient update rule](#approximate_update) above. Once again, these methods are distinguished by the target $U_t$ used. \n",
    "\n",
    "For example, the target for the approximate MC method is simply $U_t=G_t$, giving rise to 'Gradient MC' methods. The target for the approximate n-step TD method is $U_t=G_{t:t+n}$ (and similarly for n-step Sarsa). However, this bootstrapping target depends on the current value of $\\mathbf{w}$, which breaks the key assumption of the gradient update rule above. Methods that use this update are therefore called 'Semi-gradient' methods, since they ignore part of the gradient. Convergence is nonetheless assured in certain cases, for example when a linear value function approximator is used ([see below](#linear_approximation))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.3 &nbsp; Semi-gradient TD($\\lambda$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient MC and Semi-gradient n-step TD directly parallel their tabular counterparts. However, function approximation techniques also allow us to introduce something new, TD($\\lambda$). This method is a bit more subtle than those we have already met, and so requires a bit more explanation.\n",
    "\n",
    "MC and n-step TD methods takes a forward-view: values of states are updated by looking ahead to the values of future states. This works well, but means that the target is not actually available until $n$ steps into the future. This means that the updates are not equally distributed in time: our first update is delayed by $n$ steps from the start of an episode, and this leaves us $n$ updates to catch up after the episode has finished. Additionally, in order to perform each update we must maintain a store of the last $n$ feature vectors (and rewards), which comes with computational cost.\n",
    "\n",
    "TD($\\lambda$) converts these forward view methods into backward-view versions. The mechanism for this is a short-term memory vector, the eligibility trace $\\mathbf{z}_t \\in \\mathbb{R}^d$, that parallels the long-term weight vector $\\mathbf{w} \\in \\mathbb{R}^d$, keeping track of  which components of $\\mathbf{w}$ have contributed to recent state valuations. \n",
    "\n",
    "$$\\begin{align} &\\mathbf{z}_{-1} = \\mathbf{0} \\\\\n",
    " &\\mathbf{z}_t = \\gamma \\lambda \\mathbf{z}_{t-1} + \\nabla \\hat{v}(S_t, \\mathbf{w}_t), \\quad 0 \\leq t \\leq T,\\end{align}$$\n",
    " \n",
    "where $\\gamma$ is the usual discount rate, $\\lambda \\in [0, 1]$, and the product of these two quantities defines the concept of 'recent' in the description above.\n",
    "\n",
    "The trace indicates which components of $\\mathbf{w}$ deserve most credit for the error at the current state, where error is defined by the moment-by-moment one-step TD error, $\\delta_t = G_{t:t+1} - \\hat{v}(S_t, \\mathbf{w}_t)$. Components of $\\mathbf{w}$ that have contributed most recently, or most frequently to preceding state valuations are assigned the most credit, and are said to be the most 'eligible' for an update. Concretely, each component of $\\mathbf{w}$ is updated in proportion to the scalar TD error and it's corresponding component in the vector eligibility trace.\n",
    "\n",
    "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\delta_t \\mathbf{z}_t. $$\n",
    "\n",
    "In forward view n-step TD we can imagine the agent moving through state space with a telescope, looking ahead to future rewards and states in order to evaluate the currently visited state. In the backward view TD($\\lambda$), we can imagine the same agent moving through state space with a blindfold, computing the TD error at each state visited, and shouting this back to previously visited states. \n",
    "\n",
    "When $\\lambda = 0$ the trace is exactly the state-value gradient for $S_t$, and the update for $\\mathbf{w}$ reduces to the one-step semi-gradient TD update. When $\\lambda = 1$ the trace decays only according to $\\gamma$ and, although this is not as trivial to see, the update reduces to the MC Gradient update. Intermediate values of $\\lambda$ represent intermediate levels of bootstrapping between these two extremes, just as intermediate values of $n$ represent intermediate levels of bootstrapping in the n-step TD algorithm.\n",
    "\n",
    "In fact it can be shown that the TD($\\lambda$) is exactly equivalent to n-step TD, but with the n-step return $G_{t:t+n}$ replaced by a compound return composed of a weighted average of $\\textit{all}$ the n-step returns, each weighted proportional to\n",
    "$\\lambda^{n-1}$ (the same $\\lambda$ as above) and normalised by a factor of $1-\\lambda$ to ensure that the weights sum to $1$. This is called the $\\lambda$-return, $G_t^\\lambda$.\n",
    "\n",
    "$$ G_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1}G_{t:t+n}. $$\n",
    "\n",
    "This equivalence can be seen easily for the cases $\\lambda = 0$ and $\\lambda = 1$. In the former case $G_t^\\lambda$ reduces to $G_{t:t+1}$, the one-step TD return, and in the latter it reduces to $G_t$, the complete MC return, which is consistent with the behaviour of TD($\\lambda$) above.\n",
    "\n",
    "The advantages of eligibilty traces over n-step methods are clear. Updates are now performed continually and uniformly in time rather than being delayed $n$ steps and then catching up at the end of the episode (note that for the $\\lambda$-return we would actually have to wait all the way until the end of the episode to make the first update since only then is the longest of its component returns available). This means that learning can occur and affect behaviour immediately after a state is encountered rather than being delayed $n$ steps. Additionally, now only a single trace vector needs storing rather than the last n feature vectors.\n",
    "\n",
    "The formulation we have used for the eligibility trace is called the accumulating trace: each time a state is visited the corresponding components of $\\mathbf{z}$ are bumped up, decaying away between visits. Another kind of trace is called the replacing trace, which resets the components back to $1$ on each state visit rather than growing them any further (i.e. the frequency with which components of $\\mathbf{w}$ have contributed to recent state valuations is dropped as a measure of eligibility).\n",
    "\n",
    "Of course the concept of eligibility traces works for action-value function approximation methods too. As usual all that is required is to swap $\\hat{v}$s for $\\hat{q}$s in the discussion above. The resulting methods are called Sarsa($\\lambda$) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.4 &nbsp; Linear Value Function Approximation <a id='linear_approximation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important special case of value function approximation (which can be applied to all the gradient and semi-gradient methods we have encountered so far) is that for which $\\hat{v}(s, \\mathbf{w})$ is a linear function of $\\mathbf{w}$:\n",
    "\n",
    "$$ \\hat{v}(s, \\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}(s) = \\sum_i^d w_i x_i(s). $$\n",
    "\n",
    "$\\mathbf{x}(s)$ here is a featurized representation of $s$, of the same dimensions as $\\mathbf{w}$ ([see below](#feature_construction)). The gradient of the approximate value function with respect to $\\mathbf{w}$ in this linear case is simply $\\nabla \\hat{v}(s, \\mathbf{w}) = \\mathbf{x}(s)$, which reduces the SGD weight vector update to:\n",
    "\n",
    "$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\left[U_t - \\hat{v}(S_t, \\mathbf{w}_t) \\right] \\mathbf{x}(S_t). $$\n",
    "\n",
    "For action-values, the $\\hat{v}$s are replaced with $\\hat{q}$s, and $\\mathbf{x}(s)$ becomes $\\mathbf{x}(s, a)$, the featurized representation of the state-action pair $(s, a)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.5 &nbsp; Feature Construction for Linear Methods <a id='feature_construction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our states $s$ have two numerical dimensions, or features, $s_1$ and $s_2$. For example, $s_1$ could be position and $s_2$ velocity, as is the case with the Mountain Car task that we consider in part 2 of this tutorial. The most natural 'featurization' for $s$ would be $\\mathbf{x} = (s_1, s_2)^\\top$.\n",
    "\n",
    "A limitation of the linear value function approximator is that it can't take into account interactions between features, such as a high value for $s_1$ only being good in the presence of a positive value for $s_2$ in the example above. The only way to model such interdependencies would be to include in the featurization of $s$ features that combine $s_1$ and $s_2$. For example, we could use a polynomial featurization such as $\\mathbf{x}(s) = (1, s_1, s_2, s_1s_2, s_1s_2^2, s_1^2s_2, s_1^2s_2^2)^\\top$.\n",
    "\n",
    "Tile coding is another way to construct features that is more flexible and scalable than polynomial featurization. In tile coding, the state space is covered by multiple tilings, with each tiling composed of multiple tiles. The tilings are offset from one another by a fraction of the tile width. The state featurization $\\mathbf{x}(s)$ has one feature for each tile in each tiling, so if there are $n$ tilings and $k \\times k$ tiles in each tiling, $\\mathbf{x}(s) \\in \\mathbb{R}^{n \\times k \\times k}$. A particular feature is $1$ (active) if the state falls within the corresponding tile and $0$ (inactive) otherwise. Since every position in state space falls in exactly one tile in each of the $n$ tilings, there are always $n$ active features in $\\mathbf{x}(s)$. \n",
    "\n",
    "\n",
    "<img src=\"images/tile_coding.png\" width=\"600\" class=\"Center\">\n",
    "\n",
    "A nice side-effect of the above is that the learning rate $\\alpha$ in the [gradient update rule](#linear_approximation) can be set in an intuitive way. In particular, if $\\alpha=\\frac{1}{n}$, then the new estimate $\\hat{v}(s, \\mathbf{w}_{t+1}) = U_t$, regardless of the prior estimate $\\hat{v}(s, \\mathbf{w}_{t+1})$ (this follows from both the form of the gradient update rule as well as the form of $\\hat{v}(s, \\mathbf{w})$). Usually we only want to move some fraction ${x}$ of the way to the target $U_t$, which can be achieved by setting $\\alpha=\\frac{x}{n}$ instead. \n",
    "\n",
    "Tile coding ensures generalization because if we train at one state $s$, a single point in state space, then the weights associated with all ($n$) tiles covering $s$ will be updated. In turn, the approximate value function for all states within the union of these active tiles will change, proportional to the number of tiles they have in common. The shape and offset of the tiles will affect the type of generalization. For example, if the tiles are elongated along a particular dimension in state space, then generalization will extend further to states along this dimension. The number and size of the tiles will determine the finest level of discrimination possible between states - how far you have to move in state space in order to change at least one component of the featurization. \n",
    "\n",
    "Because tile coding uses binary features, the approximate value function $\\hat{v}(s, \\mathbf{w})$ in the linear case is almost trivial to compute:\n",
    "\n",
    "$$ \\hat{v}(s, \\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x}(s) = \\sum_i^d w_i x_i(s) \\equiv \\sum_{i \\in \\{x_i = 1\\}}w_i$$ \n",
    "\n",
    "In order to compute the approximate action-value function $\\hat{q}(s, a, \\mathbf{w})$, we simply need to replace $\\mathbf{x}(s)$ by $\\mathbf{x}(s, a)$. The latter can be computed by making the tilings cover the whole state-action space as opposed to just the state-space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &nbsp; 2 Implementation <a id='implementation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement n-step Sarsa and Sarsa($\\lambda$) and their associated dependencies. These implementations should be well understood by studying [Part 1](#theory) above; comments have been added to steps that may require extra explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "from collections import namedtuple\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "\n",
    "from lib.tile_coding import IHT, tiles\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env._max_episode_steps = 3000  # Increase upper time limit so we can plot full behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEstimator():\n",
    "    \"\"\"\n",
    "    Linear action-value (q-value) function approximator for \n",
    "    semi-gradient methods with state-action featurization via tile coding. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, step_size, num_tilings=8, max_size=4096, tiling_dim=None, trace=False):\n",
    "        \n",
    "        self.trace = trace\n",
    "        self.max_size = max_size\n",
    "        self.num_tilings = num_tilings\n",
    "        self.tiling_dim = tiling_dim or num_tilings\n",
    "\n",
    "        # Step size is interpreted as the fraction of the way we want \n",
    "        # to move towards the target. To compute the learning rate alpha,\n",
    "        # scale by number of tilings. \n",
    "        self.alpha = step_size / num_tilings\n",
    "\n",
    "        # Initialize index hash table (IHT) for tile coding.\n",
    "        # This assigns a unique index to each tile up to max_size tiles.\n",
    "        # Ensure max_size >= total number of tiles (num_tilings x tiling_dim x tiling_dim)\n",
    "        # to ensure no duplicates.\n",
    "        self.iht = IHT(max_size)\n",
    "\n",
    "        # Initialize weights (and optional trace)\n",
    "        self.weights = np.zeros(max_size)\n",
    "        if self.trace:\n",
    "            self.z = np.zeros(max_size)\n",
    "\n",
    "        # Tilecoding software partitions at integer boundaries, so must rescale\n",
    "        # position and velocity space to span tiling_dim x tiling_dim region.\n",
    "        self.position_scale = self.tiling_dim / (env.observation_space.high[0] \\\n",
    "                                                  - env.observation_space.low[0])\n",
    "        self.velocity_scale = self.tiling_dim / (env.observation_space.high[1] \\\n",
    "                                                  - env.observation_space.low[1])\n",
    "        \n",
    "    def featurize_state_action(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns the featurized representation for a \n",
    "        state-action pair.\n",
    "        \"\"\"\n",
    "        featurized = tiles(self.iht, self.num_tilings, \n",
    "                           [self.position_scale * state[0], \n",
    "                            self.velocity_scale * state[1]], \n",
    "                           [action])\n",
    "        return featurized\n",
    "    \n",
    "    def predict(self, s, a=None):\n",
    "        \"\"\"\n",
    "        Predicts q-value(s) using linear FA.\n",
    "        If action a is given then returns prediction\n",
    "        for single state-action pair (s, a).\n",
    "        Otherwise returns predictions for all actions \n",
    "        in environment paired with s.   \n",
    "        \"\"\"\n",
    "    \n",
    "        if a is None:\n",
    "            features = [self.featurize_state_action(s, i) for \n",
    "                        i in range(env.action_space.n)]\n",
    "        else:\n",
    "            features = [self.featurize_state_action(s, a)]\n",
    "            \n",
    "        return [np.sum(self.weights[f]) for f in features]\n",
    "        \n",
    "            \n",
    "    def update(self, s, a, target):\n",
    "        \"\"\"\n",
    "        Updates the estimator parameters\n",
    "        for a given state and action towards\n",
    "        the target using the gradient update rule \n",
    "        (and the eligibility trace if one has been set).\n",
    "        \"\"\"\n",
    "        features = self.featurize_state_action(s, a)\n",
    "        estimation = np.sum(self.weights[features])  # Linear FA\n",
    "        delta = (target - estimation)\n",
    "        \n",
    "        if self.trace:\n",
    "            # self.z[features] += 1  # Accumulating trace\n",
    "            self.z[features] = 1  # Replacing trace\n",
    "            self.weights += self.alpha * delta * self.z\n",
    "        else:\n",
    "            self.weights[features] += self.alpha * delta\n",
    "                \n",
    "    \n",
    "    def reset(self, z_only=False):\n",
    "        \"\"\"\n",
    "        Resets the eligibility trace (must be done at \n",
    "        the start of every epoch) and optionally the\n",
    "        weight vector (if we want to restart training\n",
    "        from scratch).\n",
    "        \"\"\"\n",
    "        \n",
    "        if z_only:\n",
    "            assert self.trace, 'q-value estimator has no z to reset.'\n",
    "            self.z = np.zeros(self.max_size)\n",
    "        else:\n",
    "            if self.trace:\n",
    "                self.z = np.zeros(self.max_size)\n",
    "            self.weights = np.zeros(self.max_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, epsilon, num_actions):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a \n",
    "    given q-value approximator and epsilon.    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        action_probs = np.ones(num_actions, dtype=float) * epsilon / num_actions\n",
    "        q_values = estimator.predict(observation)\n",
    "        best_action_idx = np.argmax(q_values)\n",
    "        action_probs[best_action_idx] += (1.0 - epsilon)\n",
    "        return action_probs\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_n(n, env, estimator, gamma=1.0, epsilon=0):\n",
    "    \"\"\"\n",
    "    n-step semi-gradient Sarsa algorithm\n",
    "    for finding optimal q and pi via Linear\n",
    "    FA with n-step TD updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create epsilon-greedy policy\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        estimator, epsilon, env.action_space.n)\n",
    "\n",
    "    # Reset the environment and pick the first action\n",
    "    state = env.reset()\n",
    "    action_probs = policy(state)\n",
    "    action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "\n",
    "    # Set up trackers\n",
    "    states = [state]\n",
    "    actions = [action]\n",
    "    rewards = [0.0]\n",
    "\n",
    "    # Step through episode\n",
    "    T = float('inf')\n",
    "    for t in itertools.count():\n",
    "        if t < T:           \n",
    "            # Take a step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                T = t + 1\n",
    "\n",
    "            else:\n",
    "                # Take next step\n",
    "                next_action_probs = policy(next_state)\n",
    "                next_action = np.random.choice(\n",
    "                    np.arange(len(next_action_probs)), p=next_action_probs)\n",
    "\n",
    "                actions.append(next_action)\n",
    "\n",
    "        update_time = t + 1 - n  # Specifies state to be updated\n",
    "        if update_time >= 0:       \n",
    "            # Build target\n",
    "            target = 0\n",
    "            for i in range(update_time + 1, min(T, update_time + n) + 1):\n",
    "                target += np.power(gamma, i - update_time - 1) * rewards[i]\n",
    "            if update_time + n < T:\n",
    "                q_values_next = estimator.predict(states[update_time + n])\n",
    "                target += q_values_next[actions[update_time + n]]\n",
    "            \n",
    "            # Update step\n",
    "            estimator.update(states[update_time], actions[update_time], target)\n",
    "        \n",
    "        if update_time == T - 1:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action\n",
    "    \n",
    "    ret = np.sum(rewards)\n",
    "    \n",
    "    return t, ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lambda(lmbda, env, estimator, gamma=1.0, epsilon=0):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sarsa(Lambda) algorithm\n",
    "    for finding optimal q and pi via Linear\n",
    "    FA with eligibility traces.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reset the eligibility trace\n",
    "    estimator.reset(z_only=True)\n",
    "\n",
    "    # Create epsilon-greedy policy\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        estimator, epsilon, env.action_space.n)\n",
    "\n",
    "    # Reset the environment and pick the first action\n",
    "    state = env.reset()\n",
    "    action_probs = policy(state)\n",
    "    action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "\n",
    "    ret = 0\n",
    "    # Step through episode\n",
    "    for t in itertools.count():\n",
    "        # Take a step\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        ret += reward\n",
    "\n",
    "        if done:\n",
    "            target = reward\n",
    "            estimator.update(state, action, target)\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Take next step\n",
    "            next_action_probs = policy(next_state)\n",
    "            next_action = np.random.choice(\n",
    "                np.arange(len(next_action_probs)), p=next_action_probs)\n",
    "\n",
    "            # Estimate q-value at next state-action\n",
    "            q_new = estimator.predict(\n",
    "                next_state, next_action)[0]\n",
    "            target = reward + gamma * q_new\n",
    "            # Update step\n",
    "            estimator.update(state, action, target)\n",
    "            estimator.z *= gamma * lmbda\n",
    "\n",
    "        state = next_state\n",
    "        action = next_action    \n",
    "    \n",
    "    return t, ret\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 &nbsp; Experiments <a id='experiments'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the performance of n-step Sarsa and Sarsa($\\lambda$) applied to the [Mountain Car problem](https://gym.openai.com/envs/MountainCarContinuous-v0/). This is a classic episodic task commonly used to benchmark RL algorithms, where the goal is to get a car to the top of a hill, the catch being that the engine is not strong enough on its own so the agent must learn to exploit gravity. \n",
    "\n",
    "The simulations we run use the implementations from [Part 2](#implementation). Note that for both algorithms we use the default of $\\epsilon=0$. This is OK because the Mountain Car environment yields a reward of $-1$ for each action unless it leads directly to the top of the hill (when it a yields reward of $0$). Our zero initialisation of the q-value function is therefore optimistic, and so the agent still explores (at least at the start of training). We also use the default of $\\gamma=1$ as discounting isn't strictly necessary in episodic tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 &nbsp; Plotting Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_to_go(env, estimator, num_partitions=50):\n",
    "    \"\"\"\n",
    "    Plots -Q(s, a_max) for each state s=(position, velocity) \n",
    "    in the environment where a_max is the maximising action \n",
    "    from s according to our q-value estimator Q.\n",
    "    The state-space is continuous hence we first discretise \n",
    "    it into num_partitions partitions in each dimension. \n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=num_partitions)\n",
    "    y = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=num_partitions)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.apply_along_axis(\n",
    "        lambda obs: -np.max(estimator.predict(obs)), 2, np.stack([X, Y], axis=2))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    p = ax.pcolor(X, Y, Z, cmap=cm.RdBu, vmin=0, vmax=200)\n",
    "\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Velocity')\n",
    "    ax.set_title(\"\\\"Cost To Go\\\" Function\")\n",
    "    fig.colorbar(p)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_greedy_policy(env, estimator):\n",
    "    \"\"\"\n",
    "    Follows (deterministic) greedy policy\n",
    "    with respect to the given q-value estimator,\n",
    "    and animates the result using openAI gym.\n",
    "    \"\"\"\n",
    "    # Set epsilon to zero to follow greedy policy\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        estimator=estimator, epsilon=0, num_actions=env.action_space.n)\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    for t in itertools.count():\n",
    "        time.sleep(0.01)  # Slow down animation\n",
    "        action_probs = policy(state)  # Compute action-values\n",
    "        [action] = np.nonzero(action_probs)[0]  #Â Greedy action\n",
    "        state, _, done, _ = env.step(action)  # Take step\n",
    "        env.render()  # Animate\n",
    "        if done:\n",
    "            print('Solved in {} steps'.format(t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(stats, smoothing_window=10):\n",
    "    \"\"\"\n",
    "    Plots the number of steps taken by the agent\n",
    "    to solve the task as a function of episode number,\n",
    "    smoothed over the last smoothing_window episodes. \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    for algo_stats in stats:\n",
    "        steps_per_episode = pd.Series(algo_stats.steps).rolling(\n",
    "            smoothing_window).mean()  # smooth\n",
    "        plt.plot(steps_per_episode, label=algo_stats.algorithm)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Steps\")\n",
    "    plt.title(\"Steps per Episode\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search(stats, truncate_steps=400):\n",
    "    \"\"\" \n",
    "    Plots average number of steps taken by the agent \n",
    "    to solve the task for each combination of\n",
    "    step size and boostrapping parameter\n",
    "    (n or lambda).\n",
    "    \"\"\"\n",
    "    # Truncate high step values for clearer plotting\n",
    "    stats.steps[stats.steps > truncate_steps] = truncate_steps\n",
    "    \n",
    "    # We use -1 step values indicate corresponding combination of\n",
    "    # parameters doesn't converge. Set these to truncate_steps for plotting.\n",
    "    stats.steps[stats.steps == -1] = truncate_steps\n",
    "    \n",
    "    plt.figure()\n",
    "    for b_idx in range(len(stats.bootstrappings)):\n",
    "        plt.plot(stats.step_sizes, stats.steps[b_idx, :], \n",
    "            label='Bootstrapping: {}'.format(stats.bootstrappings[b_idx]))\n",
    "    plt.xlabel('Step size (alpha * number of tilings)')\n",
    "    plt.ylabel('Average steps per episode')\n",
    "    plt.title('Grid Search {}'.format(stats.algorithm))\n",
    "    plt.ylim(140, truncate_steps - 100)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 &nbsp; Running n-step Sarsa and Sarsa($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunStats = namedtuple('RunStats', ['algorithm', 'steps', 'returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(algorithm, num_episodes=500, **algorithm_kwargs):\n",
    "    \"\"\"\n",
    "    Runs algorithm over multilple episodes and logs\n",
    "    for each episode the complete return (G_t) and the\n",
    "    number of steps taken.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = RunStats(\n",
    "        algorithm=algorithm, \n",
    "        steps=np.zeros(num_episodes), \n",
    "        returns=np.zeros(num_episodes))\n",
    "    \n",
    "    algorithm_fn = globals()[algorithm]\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        episode_steps, episode_return = algorithm_fn(**algorithm_kwargs)\n",
    "        stats.steps[i] = episode_steps\n",
    "        stats.returns[i] = episode_return\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\rEpisode {}/{} Return {}\".format(\n",
    "            i + 1, num_episodes, episode_return), end=\"\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Run n-step Sarsa}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.5  # Fraction of the way we want to move towards target\n",
    "n = 4  # Level of bootstrapping (set to intermediate value)\n",
    "num_episodes = 500\n",
    "\n",
    "estimator_n = QEstimator(step_size=step_size)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "run_stats_n = run('sarsa_n', num_episodes, n=n, env=env, estimator=estimator_n)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "\n",
    "plot_cost_to_go(env, estimator_n)\n",
    "display_greedy_policy(env, estimator_n)\n",
    "print('{} episodes completed in {:.2f}s'.format(num_episodes, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Run Sarsa}(\\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.5 # Fraction of the way we want to move towards target\n",
    "lmbda = 0.92  # Level of bootstrapping (set to intermediate value)\n",
    "num_episodes = 500\n",
    "\n",
    "estimator_lambda = QEstimator(step_size=step_size, trace=True)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "run_stats_lambda = run('sarsa_lambda', num_episodes, lmbda=lmbda, env=env, estimator=estimator_lambda)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "\n",
    "plot_cost_to_go(env, estimator_lambda)\n",
    "display_greedy_policy(env, estimator_lambda)\n",
    "print('{} episodes completed in {:.2f}s'.format(num_episodes, elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Compare Learning Curves}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves([run_stats_n, run_stats_lambda])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 &nbsp; Grid Search Comparison of n-step Sarsa and Sarsa($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchStats = namedtuple('GridSearchStats', ['algorithm', 'steps', 'step_sizes', 'bootstrappings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(algorithm, step_sizes, bootstrappings, episodes=100, num_runs=5,\n",
    "                   **algorithm_kwargs):\n",
    "    \"\"\"\n",
    "    Runs a grid search over different values of the step size\n",
    "    and boostrapping parameter (n or lambda) for the given algorithm.\n",
    "    The performance of each combination of parameters is measured \n",
    "    by the number of steps taken to complete the task, averaged\n",
    "    over the first few episodes and a number of\n",
    "    independent runs.\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = GridSearchStats(\n",
    "        algorithm=algorithm, \n",
    "        steps=np.zeros((len(bootstrappings), len(step_sizes))),\n",
    "        step_sizes=step_sizes,\n",
    "        bootstrappings=bootstrappings)\n",
    "        \n",
    "    algorithm_fn = globals()[algorithm]\n",
    "    trace = True if algorithm == 'sarsa_lambda' else False\n",
    "\n",
    "    for run_idx in range(num_runs):\n",
    "        for b_idx, bootstrapping in enumerate(bootstrappings):\n",
    "            for s_idx, step_size in enumerate(step_sizes):\n",
    "                if algorithm == 'sarsa_n':\n",
    "                    if (bootstrapping == 8 and step_size > 1) or \\\n",
    "                    (bootstrapping == 16 and step_size > 0.75):\n",
    "                        # sarsa_n doesn't converge in these cases so \n",
    "                        # assign a default value and skip over.\n",
    "                        stats.steps[b_idx, s_idx] = -1 * num_runs * episodes\n",
    "                    continue\n",
    "                for episode in range(episodes):\n",
    "                    sys.stdout.flush()\n",
    "                    print('\\r run: {}, step_size: {}, bootstrapping: {}, episode: {}'.format(\n",
    "                            run_idx, step_size, bootstrapping, episode), end=\"\")\n",
    "                    episode_steps, _ = algorithm_fn(\n",
    "                        bootstrapping, estimator=estimator, **algorithm_kwargs)\n",
    "                    stats.steps[b_idx, s_idx] += episode_steps\n",
    "                    \n",
    "    \n",
    "    # Average over independent runs and episodes\n",
    "    stats.steps[:] /= (num_runs * episodes)\n",
    "   \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Grid Search for n-step Sarsa}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " run: 0, step_size: 1.7000000000000002, bootstrapping: 16, episode: 69"
     ]
    },
    {
     "ename": "RuntimeWarning",
     "evalue": "overflow encountered in reduce",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeWarning\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9411af2736fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstep_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgrid_search_stats_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sarsa_n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search_stats_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-1444d67357b0>\u001b[0m in \u001b[0;36mrun_grid_search\u001b[0;34m(algorithm, step_sizes, bootstrappings, episodes, num_runs, **algorithm_kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m                             run_idx, step_size, bootstrapping, episode), end=\"\")\n\u001b[1;32m     30\u001b[0m                     episode_steps, _ = algorithm_fn(\n\u001b[0;32m---> 31\u001b[0;31m                         bootstrapping, estimator=estimator, **algorithm_kwargs)\n\u001b[0m\u001b[1;32m     32\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mepisode_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-80fcd38dcd67>\u001b[0m in \u001b[0;36msarsa_n\u001b[0;34m(n, env, estimator, gamma, epsilon)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# Take next step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mnext_action_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 next_action = np.random.choice(\n\u001b[1;32m     38\u001b[0m                     np.arange(len(next_action_probs)), p=next_action_probs)\n",
      "\u001b[0;32m<ipython-input-4-23373f39d6a5>\u001b[0m in \u001b[0;36mpolicy_fn\u001b[0;34m(observation)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mbest_action_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maction_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_action_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-eb8bb9881bc1>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize_state_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-eb8bb9881bc1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize_state_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RL/venv/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1880\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1881\u001b[0m     return _methods._sum(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 1882\u001b[0;31m                          out=out, **kwargs)\n\u001b[0m\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/RL/venv/lib/python3.5/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeWarning\u001b[0m: overflow encountered in reduce"
     ]
    }
   ],
   "source": [
    "step_sizes = np.arange(0.1, 1.8, 0.1)\n",
    "ns = np.power(2, np.arange(0, 5))\n",
    "grid_search_stats_n = run_grid_search('sarsa_n', step_sizes, ns, env=env)\n",
    "plot_grid_search(grid_search_stats_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Grid Search for Sarsa}(\\lambda)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = np.arange(0.1, 1.8, 0.1)\n",
    "lambdas = np.array([0, 0.68, 0.84, 0.92, 0.98, 0.99])\n",
    "grid_search_stats_lambda = run_grid_search('sarsa_lambda', step_sizes, lambdas, env=env)\n",
    "plot_grid_search(grid_search_stats_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 &nbsp; Conclusion <a id='conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial has covered the theory and implementation of two important algorithms in RL, n-step Sarsa and Sarsa($\\lambda$). Judging by our experiments in [Part 2](#experiments), Sarsa($\\lambda$) appears to learn more efficiently than n-step Sarsa as applied to the Mountain Car task. The agent's ability to immediately learn from and respond to feedback from the environment rather than being delayed n-steps seems to pay off. \n",
    "\n",
    "By studying these two algorithms, we have covered most of the key areas of RL. However, there are two important areas that we didn't cover and that is off-policy methods (although we explained at a high level what this was in [section 1.5](#on_off_policy)) and policy gradient methods. Both n-step Sarsa and Sarsa($\\lambda$) have off-policy equivalents (so called Q-learning methods) that simulate sample experience from a behaviour policy different from the target policy we are actually learning about, but are otherwise the same as their on-policy counterparts. Policy gradient methods, on the other hand, introduce something entirely new. In these methods, optimal policies are not learned via value functions, but by first parameterising the policy and then optimising it directly via stochastic gradient descent, without consulting a value function at all. Off-policy methods and policy gradient methods are also well worth studying, as many state-of-the-art applications of RL employ such techniques.\n",
    "\n",
    "Of course we also haven't covered Deep Learning RL methods (such as Deep Q-Learning). However, these are essentially just methods that replace the linear function approximator that we have used with a neural network (we can also do away with tile coding since neural networks are non-linear function approximators already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
